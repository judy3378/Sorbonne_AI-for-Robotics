{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bfae0f4",
   "metadata": {},
   "source": [
    " Copyright © Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8500196b",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, we will implement the REINFORCE algorithm using BBRL.\n",
    "\n",
    "To understand this code, you need to know more about [the BBRL interaction\n",
    "model](https://github.com/osigaud/bbrl/blob/master/docs/overview.md) Then you\n",
    "should run [a didactical\n",
    "example](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/02-multi_env_noautoreset.student.ipynb)\n",
    "to see how agents interact in BBRL when autoreset=False.\n",
    "\n",
    "The REINFORCE algorithm is explained in a series of 3 videos: [video\n",
    "1](https://www.youtube.com/watch?v=R7ULMBXOQtE), [video\n",
    "2](https://www.youtube.com/watch?v=dKUWto9B9WY) and [video\n",
    "3](https://www.youtube.com/watch?v=GcJ9hl3T6x8). You can also read the\n",
    "corresponding slides:\n",
    "[slides1](http://pages.isir.upmc.fr/~sigaud/teach/ps/3_pg_derivation1.pdf),\n",
    "[slides2](http://pages.isir.upmc.fr/~sigaud/teach/ps/4_pg_derivation2.pdf),\n",
    "[slides3](http://pages.isir.upmc.fr/~sigaud/teach/ps/5_pg_derivation3.pdf)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b79d455",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup()\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpisodicAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "from bbrl_utils.nn import copy_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ef595a",
   "metadata": {},
   "source": [
    "# Learning environment\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The learning environment is controlled by a configuration that define a few\n",
    "important things as described in the example below. This configuration can\n",
    "hold as many extra information as you need, the example below is the minimal\n",
    "one.\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    # This defines the a path for logs and saved models\n",
    "    \"base_dir\": \"${gym_env.env_name}/myalgo_${current_time:}\",\n",
    "\n",
    "    # The Gymnasium environment\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "\n",
    "    # Algorithm\n",
    "    \"algorithm\": {\n",
    "        # Seed used for the random number generator\n",
    "        \"seed\": 1023,\n",
    "\n",
    "        # Number of parallel training environments\n",
    "        \"n_envs\": 8,\n",
    "                \n",
    "        # Minimum number of steps between two evaluations\n",
    "        \"eval_interval\": 500,\n",
    "        \n",
    "        # Number of parallel evaluation environments\n",
    "        \"nb_evals\": 10,\n",
    "\n",
    "        # Number of epochs (loops)\n",
    "        \"max_epochs\": 40000,\n",
    "\n",
    "    },\n",
    "}\n",
    "\n",
    "# Creates the configuration object, i.e. cfg.algorithm.nb_evals is 10\n",
    "cfg = OmegaConf.create(params)\n",
    "```\n",
    "\n",
    "## The RL algorithm\n",
    "\n",
    "In this notebook, the RL algorithm is based on `EpisodicAlgo`, that defines\n",
    "the algorithm environment when using episodes. To use such environment, we\n",
    "just need to subclass `EpisodicAlgo` and to define two things, namely the\n",
    "`train_policy` and the `eval_policy`. Both are BBRL agents that, given the\n",
    "environment state, select the action to perform.\n",
    "\n",
    "```py\n",
    "  class MyAlgo(EpisodicAlgo):\n",
    "      def __init__(self, cfg):\n",
    "          super().__init__(cfg)\n",
    "\n",
    "          # Define the train and evaluation policies\n",
    "          # (the agents compute the workspace `action` variable)\n",
    "          self.train_policy = MyPolicyAgent(...)\n",
    "          self.eval_policy = MyEvalAgent(...)\n",
    "\n",
    "algo = MyAlgo(cfg)\n",
    "```\n",
    "\n",
    "The `EpisodicAlgo` defines useful objects:\n",
    "\n",
    "- `algo.cfg` is the configuration\n",
    "- `algo.nb_steps` (integer) is the number of steps since the training began\n",
    "- `algo.logger` is a logger that can be used to collect statistics during training:\n",
    "    - `algo.logger.add_log(\"critic_loss\", critic_loss, algo.nb_steps)` registers the `critic_loss` value on tensorboard\n",
    "- `algo.evaluate()` evaluates the current `eval_policy` if needed, and keeps the\n",
    "agent if it was the best so far (average cumulated reward);\n",
    "- `algo.visualize_best()` runs the best agent on one episode, and displays the video\n",
    "\n",
    "\n",
    "\n",
    "Besides, it also defines an `iter_episodes` is simple:\n",
    "\n",
    "```py\n",
    "  # With episodes\n",
    "  for workspace in rl_algo.iter_episodes():\n",
    "      # workspace is a workspace containing transitions\n",
    "      # Episodes shorter than the longer one contain duplicated\n",
    "      # transitions (with `env/done` set to true)\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5853b8",
   "metadata": {},
   "source": [
    "## Definition of agents\n",
    "\n",
    "The [REINFORCE](https://link.springer.com/content/pdf/10.1007/BF00992696.pdf)\n",
    "uses a stochastic policy and a baseline which is the value function. Thus we\n",
    "need an Actor agent, a Critic agent and an Environment agent. The actor agents\n",
    "are built on an intermediate `ProbAgent`. Two agents that use the output of\n",
    "`ProbAgent` are defined below:\n",
    "- `ArgmaxActorAgent` that selects the action with the highest probability\n",
    "- `StochasticActorAgent` that selects the action using the probability\n",
    "  distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21880c8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ProbAgent(Agent):\n",
    "    # Computes the distribution $p(a_t|s_t)$\n",
    "\n",
    "    def __init__(self, state_dim, hidden_layers, n_action, name=\"prob_agent\"):\n",
    "        super().__init__(name)\n",
    "        self.model = build_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [n_action], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        # Get $s_t$\n",
    "        observation = self.get((\"env/env_obs\", t))\n",
    "        # Compute the distribution over actions\n",
    "        scores = self.model(observation)\n",
    "        action_probs = torch.softmax(scores, dim=-1)\n",
    "        assert not torch.any(torch.isnan(action_probs)), \"NaN Here\"\n",
    "\n",
    "        self.set((\"action_probs\", t), action_probs)\n",
    "        entropy = torch.distributions.Categorical(action_probs).entropy()\n",
    "        self.set((\"entropy\", t), entropy)\n",
    "\n",
    "\n",
    "class StochasticActorAgent(Agent):\n",
    "    \"\"\"Sample an action according to $p(a_t|s_t)$\"\"\"\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        probs = self.get((\"action_probs\", t))\n",
    "        action = torch.distributions.Categorical(probs).sample()\n",
    "        self.set((\"action\", t), action)\n",
    "\n",
    "\n",
    "class ArgmaxActorAgent(Agent):\n",
    "    \"\"\"Choose an action $a$ that maximizes $p(a_t|s_t)\"\"\"\n",
    "\n",
    "    def forward(self, t: int, *, stochastic: bool = None, **kwargs):\n",
    "        probs = self.get((\"action_probs\", t))\n",
    "        action = probs.argmax(1)\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b95aef",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### VAgent\n",
    "\n",
    "The VAgent is a neural network which takes an observation as input and whose\n",
    "output is the value $V(s)$ of this observation. This is useful to\n",
    "reduce the bias on the estimation of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2d1ca8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class VAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers):\n",
    "        super().__init__()\n",
    "        self.is_q_function = False\n",
    "        self.model = build_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        observation = self.get((\"env/env_obs\", t))\n",
    "        # The `squeeze(-1)` removes the last dimension of the tensor.\n",
    "        # (since this is a scalar, we want to ignore this dimension since\n",
    "        # the target values will also be scalars)\n",
    "        critic = self.model(observation).squeeze(-1)\n",
    "        self.set((\"v_value\", t), critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d1e3722",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### RL environment\n",
    "\n",
    "In the next cell, we define the Reinforce environment. It is based on `EpisodicAlgo`\n",
    "since learning uses full episodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864f29f9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class Reinforce(EpisodicAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "\n",
    "        # Train and critic agents\n",
    "        self.proba_agent = ProbAgent(\n",
    "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "        )\n",
    "        self.train_policy = Agents(self.proba_agent, StochasticActorAgent())\n",
    "\n",
    "        # The critic/value agent (if used)\n",
    "        self.t_critic_agent = TemporalAgent(\n",
    "            VAgent(obs_size, cfg.algorithm.architecture.critic_hidden_size)\n",
    "        )\n",
    "\n",
    "        # Evaluation policy\n",
    "        self.eval_policy = Agents(self.proba_agent, ArgmaxActorAgent())\n",
    "\n",
    "        # Setup the optimizer\n",
    "        self.optimizer = setup_optimizer(\n",
    "            cfg.optimizer, self.proba_agent, self.t_critic_agent\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1760d1",
   "metadata": {},
   "source": [
    "The next cell describes the arguments of the two main arguments used in the\n",
    "training function `run`:\n",
    "- `compute_advantage` computes the reward at each time step\n",
    "- `compute_critic_loss` computes the loss of the critic (if we use one), i.e.\n",
    "  the baseline in reinforce\n",
    "\n",
    "Both functions will be implemented depending on the reinforce flavor,\n",
    "so you can leave them empty here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3124bb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_advantage(\n",
    "    cfg, reward: torch.Tensor, v_value: torch.Tensor\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"Computes the reward at each episode step\n",
    "    \n",
    "    This function is passed as a parameter, and depend on the Reinforce flavor –\n",
    "    the function in this notebook cell will thus never been called.\n",
    "\n",
    "    :param reward: The rewards from the environment (tensor TxB)\n",
    "    :param v_value: The values $V(s)$ computed by the critic (tensor TxB). It\n",
    "        can None if there is no baseline.\n",
    "    :returns: The reward (tensor TxB)\n",
    "    \"\"\"\n",
    "    ...\n",
    "\n",
    "\n",
    "def compute_critic_loss(\n",
    "    cfg, reward, must_bootstrap, done, v_value\n",
    ") -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n",
    "    \"\"\"Compute the critic loss\n",
    "\n",
    "    :param reward: The reward from the environment (TxB)\n",
    "    :param must_bootstrap: Whether the critic should be bootstrapped (TxB)\n",
    "    :param done: Whether the episode was finished or not at $t$ (TxB)\n",
    "    :param v_value: The v value computed by the critic ($TxB$)\n",
    "    :return: The scalar loss\n",
    "    \"\"\"\n",
    "    # By default, we don't have any critic\n",
    "    # (so just do nothing)\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c0f9e4",
   "metadata": {},
   "source": [
    "You can now write the main learning loop, based on the two above functions\n",
    "(compute_critic_loss can be None if no baseline is used).\n",
    "\n",
    "$$\n",
    "\\nabla \\mathbb{E}_\\tau(R(\\tau))\n",
    "\\approx\n",
    "\\frac{1}{m} \\sum_{i=1}^m \\frac{1}{H_i-1} \\sum_{t=1}^{H_i-1} A(s_t^{(i)})\n",
    "\\nabla  \\log p(a_k^{(i)} | s_k^{(i)})\n",
    "\n",
    "$$\n",
    "\n",
    "with $m$ the number of episodes to estimate the gradient, and $H_i$\n",
    "the number of steps in the episode, and $A$ is the advantage\n",
    "function `compute_advantage`.\n",
    "\n",
    "You also need to use `compute_critic_loss` if a baseline is learned\n",
    "to reduce the variance of the gradient estimator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5bf19fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(reinforce: Reinforce, compute_advantage, compute_critic_loss=None):\n",
    "    # We work on full episodes here\n",
    "    for train_workspace in reinforce.iter_episodes():\n",
    "        # Get relevant tensors (size are time x n_envs x ....)\n",
    "        terminated, done, action_probs, reward, action = train_workspace[\n",
    "            \"env/terminated\",\n",
    "            \"env/done\",\n",
    "            \"action_probs\",\n",
    "            \"env/reward\",\n",
    "            \"action\",\n",
    "        ]\n",
    "        must_bootstrap = ~terminated\n",
    "        \n",
    "        # Implement the main learning loop\n",
    "\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        reinforce.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f0b9b18",
   "metadata": {},
   "source": [
    "## Definition of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bf0c8af",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# We first setup tensorboard (it is better to choose \"no\" when running on your\n",
    "# own computer)\n",
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cbc85eb",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"base_dir\": \"${gym_env.env_name}/reinforce-${variant}-S${algorithm.seed}_${current_time:}\",\n",
    "    \"algorithm\": {\n",
    "        # Number of transitions between two evaluations\n",
    "        \"eval_interval\": 1000,\n",
    "        \"seed\": 1,\n",
    "        \"n_envs\": 8,\n",
    "        \"nb_evals\": 10,\n",
    "        \"max_epochs\": 700,\n",
    "        \"discount_factor\": 0.99,\n",
    "        \"critic_coef\": 1.0,\n",
    "        \"actor_coef\": 1.0,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [32],\n",
    "            \"critic_hidden_size\": [36],\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPole-v1\",\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 0.001,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efc1dfd4",
   "metadata": {},
   "source": [
    "### First algorithm: summing all the rewards along an episode\n",
    "\n",
    "The most basic variant of the Policy Gradient algorithms just sums all the\n",
    "rewards along an episode.\n",
    "\n",
    "This is implemented with the `apply_sum` function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4dd301e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_sum(cfg, reward, *args):\n",
    "    reward_sum = reward.sum(axis=0)\n",
    "    reward = torch.zeros_like(reward)\n",
    "    for i in range(len(reward)):\n",
    "        reward[i] = reward_sum\n",
    "    return reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb96d149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Runs and visualize\n",
    "reinforce_sum = Reinforce(OmegaConf.create({**params, \"variant\": \"sum\"}))\n",
    "run(reinforce_sum, apply_sum)\n",
    "reinforce_sum.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109d9c58",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "### First algorithm: summing discounted rewards\n",
    "\n",
    "As explained in the [second\n",
    "video](https://www.youtube.com/watch?v=dKUWto9B9WY) and [the corresponding\n",
    "slides](http://pages.isir.upmc.fr/~sigaud/teach/ps/4_pg_derivation2.pdf),\n",
    "using a discounted reward after the current step and ignoring the rewards\n",
    "before the current step results in lower variance.\n",
    "\n",
    "By taking inspiration from the `apply_sum()` function above, code a function\n",
    "`apply_discounted_sum()` that computes the sum of discounted rewards from\n",
    "immediate rewards.\n",
    "\n",
    "Two hints:\n",
    "- you should proceed backwards, starting from the final step of the episode\n",
    "  and storing the previous sum into a register\n",
    "- you need the discount factor as an input to your function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb67295",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_discounted_sum(cfg, reward, v_value):\n",
    "    # Implement the function (ignore v_value)\n",
    "\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead5b09",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reinforce_dsum = Reinforce(OmegaConf.create({**params, \"variant\": \"dsum\"}))\n",
    "run(reinforce_dsum, apply_discounted_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04e3f552",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "reinforce_dsum.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a798e1b",
   "metadata": {},
   "source": [
    "### Second algorithm: Baseline with Temporal Differences\n",
    "\n",
    "Here, we aim at computing a baseline using temporal differences. The algorithm\n",
    "for computing the critic loss is given below.\n",
    "\n",
    "Note the `critic[1:].detach()` in the computation of the temporal difference\n",
    "target. The idea is that we compute this target as a function of $V(s_{t+1})$,\n",
    "but we do not want to apply gradient descent on this $V(s_{t+1})$, we will\n",
    "only apply gradient descent to the $V(s_t)$ according to this target value.\n",
    "\n",
    "In practice, `x.detach()` detaches a computation graph from a tensor, so it\n",
    "avoids computing a gradient over this tensor.\n",
    "\n",
    "Note also the trick to deal with terminal states. If the state is terminal,\n",
    "$V(s_{t+1})$ does not make sense. Thus we need to ignore this term. So we\n",
    "multiply the term by `must_bootstrap`: if `must_bootstrap` is True (converted\n",
    "into an int, it becomes a 1), we get the term. If `must_bootstrap` is False\n",
    "(=0), we are at a terminal state, so we ignore the term. This trick is used in\n",
    "many RL libraries, e.g. SB3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b406a76",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "Code a `apply_discounted_sum_minus_baseline()` function, using the critic\n",
    "learned simultaneously with the policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54b4bcf",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def apply_discounted_sum_minus_baseline(cfg, reward, v_value):\n",
    "    # Implement the function\n",
    "\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986c6f68",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "(2) Code a `compute_critic_loss()` using temporal differences (bootstrapped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b997996",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_td_critic_loss(cfg, reward, must_bootstrap, done, critic):\n",
    "    # To be completed...\n",
    "\n",
    "    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "    return critic_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90f95a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reinforce_td = Reinforce(OmegaConf.create({**params, \"variant\": \"td\"}))\n",
    "run(reinforce_td, apply_discounted_sum_minus_baseline, compute_td_critic_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637ccf5b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "reinforce_td.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4798636",
   "metadata": {},
   "source": [
    "### Third algorithm: Monte-Carlo Baseline\n",
    "\n",
    " The `compute_critic_loss()` function above uses the Temporal Difference\n",
    " approach to critic estimation. In this part, we will compare it to using the\n",
    " Monte Carlo estimation approach.\n",
    "\n",
    "As explained in [this video](https://www.youtube.com/watch?v=GcJ9hl3T6x8) and\n",
    "[these\n",
    "slides](http://pages.isir.upmc.fr/~sigaud/teach/ps/5_pg_derivation3.pdf), the\n",
    "MC estimation approach uses the following equation:\n",
    "\n",
    "$$\\phi_{j+1} = \\mathop{\\mathrm{argmin}}_{\\phi_j} \\frac{1}{m\\times\n",
    "   H}\\sum_{i=1}^m \\sum_{t=1}^H \\left( \\left(\\sum_{k=t}^H \\gamma^{k-t}\n",
    "   r(s_k^{(i)},a_k^{(i)}) \\right) - \\hat{V}^\\pi_{\\phi_j}(s_t^{(i)}) \\right)^2\n",
    "       $$\n",
    "\n",
    "The innermost sum of discounted rewards exactly corresponds to the computation\n",
    "of the `apply_discounted_sum()` function. The rest just consists in computing\n",
    "the squared difference (also known as the Means Squared Error, or MSE) over\n",
    "the $m \\times H$ samples ($m$ episodes of lenght $H$) that we have collected.\n",
    "\n",
    "From the above information, create a `compute_critic_loss()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab8cec8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_critic_loss_mc(cfg, reward, must_bootstrap, done, critic):\n",
    "    # To be completed...\n",
    "\n",
    "    assert False, 'Not implemented yet'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07be1d99",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reinforce_mc = Reinforce(OmegaConf.create({**params, \"variant\": \"mc\"}))\n",
    "run(reinforce_mc, apply_discounted_sum_minus_baseline, compute_critic_loss_mc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c0f7cd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Visualization\n",
    "\n",
    "reinforce_mc.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca01bfd",
   "metadata": {},
   "source": [
    "Most probably, this will not work well, as initially the learned critic is a\n",
    "poor estimate of the true $V(s)$. Instead, load an already trained critic that\n",
    "you have saved after convergence from a previous run, and see if it works\n",
    "better.\n",
    "\n",
    "Loading and saving a network or a BBRL agent can easily be performed using\n",
    "`agent.save(filename)` and `agent.load(filename)`.\n",
    "\n",
    "Warning: Be cautious with the use of ProbAgent with just a hidden layer,\n",
    "ProbAgent with build_mlp, and DiscreteActor. Try to be progressive..."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
