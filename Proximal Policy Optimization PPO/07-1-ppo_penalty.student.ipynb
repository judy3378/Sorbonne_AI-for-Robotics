{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "601857c3",
   "metadata": {},
   "source": [
    " Copyright Â© Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd4191e",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook we code one version of the [Proximal Policy Optimization\n",
    "(PPO)](https://arxiv.org/pdf/1707.06347.pdf) algorithms using BBRL. More\n",
    "precisely, the version here is the one that uses the KL penalty as a\n",
    "regularization term when optimizing the policy gradient.\n",
    "\n",
    "The PPO algorithm is superficially explained in [this\n",
    "video](https://www.youtube.com/watch?v=uRNL93jV2HE) and you can also read [the\n",
    "corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/ps/10_ppo.pdf).\n",
    "\n",
    "It is also a good idea to have a look at the [spinning up\n",
    "documentation](https://spinningup.openai.com/en/latest/algorithms/ppo.html).\n",
    "\n",
    "This version of PPO works, but it incorrectly samples minibatches randomly\n",
    "from the rollouts without making sure that each sample is used once and only\n",
    "once See:\n",
    "https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/ for a\n",
    "full description of all the coding tricks that should be integrated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61199fbe",
   "metadata": {},
   "source": [
    "# Setting up the environment\n",
    "We first need to setup the environment\n",
    "Installs the necessary Python and system libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b8e381",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "    assert run([\"pip\", \"install\", \"easypip\"]).returncode == 0, \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup()\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, KWAgentWrapper, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpisodicAlgo, iter_partial_episodes\n",
    "from bbrl_utils.nn import build_ortho_mlp, setup_optimizer\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from bbrl_utils.nn import copy_parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef7f7faf",
   "metadata": {},
   "source": [
    "# Learning environment\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The learning environment is controlled by a configuration that define a few\n",
    "important things as described in the example below. This configuration can\n",
    "hold as many extra information as you need, the example below is the minimal\n",
    "one.\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    # This defines the a path for logs and saved models\n",
    "    \"base_dir\": \"${gym_env.env_name}/myalgo_${current_time:}\",\n",
    "\n",
    "    # The Gymnasium environment\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "\n",
    "    # Algorithm\n",
    "    \"algorithm\": {\n",
    "        # Seed used for the random number generator\n",
    "        \"seed\": 1023,\n",
    "\n",
    "        # Number of parallel training environments\n",
    "        \"n_envs\": 8,\n",
    "                \n",
    "        # Minimum number of steps between two evaluations\n",
    "        \"eval_interval\": 500,\n",
    "        \n",
    "        # Number of parallel evaluation environments\n",
    "        \"nb_evals\": 10,\n",
    "\n",
    "        # Number of epochs (loops)\n",
    "        \"max_epochs\": 40000,\n",
    "\n",
    "        # Number of steps (partial iteration)\n",
    "        \"n_steps\": 100,\n",
    "        \n",
    "    },\n",
    "}\n",
    "\n",
    "# Creates the configuration object, i.e. cfg.algorithm.nb_evals is 10\n",
    "cfg = OmegaConf.create(params)\n",
    "```\n",
    "\n",
    "## The RL algorithm\n",
    "\n",
    "In this notebook, the RL algorithm is based on `EpisodicAlgo`, that defines\n",
    "the algorithm environment when using episodes. To use such environment, we\n",
    "just need to subclass `EpisodicAlgo` and to define two things, namely the\n",
    "`train_policy` and the `eval_policy`. Both are BBRL agents that, given the\n",
    "environment state, select the action to perform.\n",
    "\n",
    "```py\n",
    "  class MyAlgo(EpisodicAlgo):\n",
    "      def __init__(self, cfg):\n",
    "          super().__init__(cfg)\n",
    "\n",
    "          # Define the train and evaluation policies\n",
    "          # (the agents compute the workspace `action` variable)\n",
    "          self.train_policy = MyPolicyAgent(...)\n",
    "          self.eval_policy = MyEvalAgent(...)\n",
    "\n",
    "algo = MyAlgo(cfg)\n",
    "```\n",
    "\n",
    "The `EpisodicAlgo` defines useful objects:\n",
    "\n",
    "- `algo.cfg` is the configuration\n",
    "- `algo.nb_steps` (integer) is the number of steps since the training began\n",
    "- `algo.logger` is a logger that can be used to collect statistics during training:\n",
    "    - `algo.logger.add_log(\"critic_loss\", critic_loss, algo.nb_steps)` registers the `critic_loss` value on tensorboard\n",
    "- `algo.evaluate()` evaluates the current `eval_policy` if needed, and keeps the\n",
    "agent if it was the best so far (average cumulated reward);\n",
    "- `algo.visualize_best()` runs the best agent on one episode, and displays the video\n",
    "\n",
    "\n",
    "\n",
    "Besides, it also defines an `iter_episodes` that allows to iterate over partial\n",
    "episodes (with `n_steps` from `n_envs` environments):\n",
    "\n",
    "```python3\n",
    "  # with partial episodes\n",
    "  for workspace in algo.iter_partial_episodes():\n",
    "      # workspace is a workspace containing 50 transitions\n",
    "      # (with autoreset)\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab25377",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Definition of PPO agents\n",
    "\n",
    "### Critic agent\n",
    "\n",
    "As A2C, PPO uses a value function $V(s)$. We thus call upon the `VAgent`\n",
    "class,  which takes an observation as input and whose output is the value of\n",
    "this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19b2d24",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class VAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, name=\"critic\"):\n",
    "        super().__init__(name=name)\n",
    "        self.is_q_function = False\n",
    "        self.model = build_ortho_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        observation = self.get((\"env/env_obs\", t))\n",
    "        critic = self.model(observation).squeeze(-1)\n",
    "        self.set((f\"{self.prefix}v_values\", t), critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893a3ece",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### KL penalty agent\n",
    "\n",
    "When computing the KL penalty, we need to compute the KL divergence at every\n",
    "time step. The KLAgent is specific to the KL regularization version of PPO. It\n",
    "is used to compute the KL divergence between the current and the past policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296383a1",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class KLAgent(Agent):\n",
    "    def __init__(self, model_1, model_2):\n",
    "        super().__init__()\n",
    "        self.model_1 = model_1\n",
    "        self.model_2 = model_2\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        dist_1 = self.model_1.dist(obs)\n",
    "        dist_2 = self.model_2.dist(obs)\n",
    "        kl = torch.distributions.kl.kl_divergence(dist_1, dist_2)\n",
    "        self.set((\"kl\", t), kl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4828d0b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### The DiscretePolicy\n",
    "\n",
    "The DiscretePolicy was already used in A2C to deal with discrete actions, but\n",
    "we have added the possibility to only predict the probability of an action\n",
    "using the ```predict_proba``` variable in the ```forward()``` function. The\n",
    "code is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ded801",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class DiscretePolicy(Agent):\n",
    "    def __init__(self, state_dim, hidden_size, n_actions, name=\"policy\"):\n",
    "        super().__init__(name=name)\n",
    "        self.model = build_ortho_mlp(\n",
    "            [state_dim] + list(hidden_size) + [n_actions], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def dist(self, obs):\n",
    "        scores = self.model(obs)\n",
    "        probs = torch.softmax(scores, dim=-1)\n",
    "        return torch.distributions.Categorical(probs)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        t,\n",
    "        *,\n",
    "        stochastic=True,\n",
    "        predict_proba=False,\n",
    "        compute_entropy=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute the action given either a time step (looking into the workspace)\n",
    "        or an observation (in kwargs)\n",
    "        \"\"\"\n",
    "        observation = self.get((\"env/env_obs\", t))\n",
    "        scores = self.model(observation)\n",
    "        probs = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        if predict_proba:\n",
    "            action = self.get((\"action\", t))\n",
    "            log_probs = probs[torch.arange(probs.size()[0]), action].log()\n",
    "            self.set((f\"{self.prefix}logprob_predict\", t), log_probs)\n",
    "        else:\n",
    "            if stochastic:\n",
    "                action = torch.distributions.Categorical(probs).sample()\n",
    "            else:\n",
    "                action = scores.argmax(1)\n",
    "\n",
    "            self.set((\"action\", t), action)\n",
    "\n",
    "        if compute_entropy:\n",
    "            entropy = torch.distributions.Categorical(probs).entropy()\n",
    "            self.set((f\"{self.prefix}entropy\", t), entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ebad09",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Main PPO agent\n",
    "\n",
    "In the following, we create the PPO Agent, with one policy and one critic,\n",
    "and their \"delayed\" versions (target network for the critic, and previous \n",
    "policy in the inner loop of the optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "353eee9d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class PPOPenalty(EpisodicAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg, autoreset=True)\n",
    "\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "        self.train_policy = globals()[cfg.algorithm.policy_type](\n",
    "            obs_size,\n",
    "            cfg.algorithm.architecture.actor_hidden_size,\n",
    "            act_size,\n",
    "        ).with_prefix(\"current_policy/\")\n",
    "\n",
    "        # When evaluating, use specific keyword parameters\n",
    "        # (deterministic, no entropy, no probability over actions)\n",
    "        self.eval_policy = KWAgentWrapper(\n",
    "            self.train_policy, \n",
    "            stochastic=False,\n",
    "            predict_proba=False,\n",
    "            compute_entropy=False,\n",
    "        )\n",
    "\n",
    "        self.critic_agent = VAgent(\n",
    "            obs_size, cfg.algorithm.architecture.critic_hidden_size\n",
    "        ).with_prefix(\"critic/\")\n",
    "        self.old_critic_agent = copy.deepcopy(self.critic_agent).with_prefix(\"old_critic/\")\n",
    "        self.t_all_critics = TemporalAgent(\n",
    "            Agents(self.critic_agent, self.old_critic_agent)\n",
    "        )\n",
    "\n",
    "        self.old_policy = copy.deepcopy(self.train_policy)\n",
    "        self.old_policy.with_prefix(\"old_policy/\")\n",
    "        \n",
    "        self.t_kl_agent = TemporalAgent(KLAgent(self.old_policy, self.train_policy))\n",
    "\n",
    "        self.policy_optimizer = setup_optimizer(\n",
    "            cfg.optimizer, self.train_policy\n",
    "        )\n",
    "        self.critic_optimizer = setup_optimizer(\n",
    "            cfg.optimizer, self.critic_agent\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82beac5c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Main PPO loop\n",
    "\n",
    "In the cell below, we optimize the policy loss for PPO-KL, i.e.\n",
    "$$\n",
    "\\max_{\\theta} \\hat{\\mathbb{E}} _t\\left[\\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)} \\hat{A}_t-\\beta \\operatorname{KL}\\left[\\pi_{\\theta_{\\text {old }}}\\left(\\cdot \\mid s_t\\right), \\pi_\\theta\\left(\\cdot \\mid s_t\\right)\\right]\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3f6d13e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "from bbrl.utils.functional import gae\n",
    "\n",
    "\n",
    "def run_ppo_penalty(ppo: PPOPenalty):\n",
    "    cfg = ppo.cfg\n",
    "\n",
    "    # The old_policy params must be wrapped into a TemporalAgent\n",
    "    t_old_policy = TemporalAgent(ppo.old_policy)\n",
    "\n",
    "    # Training loop\n",
    "    for train_workspace in ppo.iter_partial_episodes():\n",
    "        # Run the current policy and evaluate the proba of its action according\n",
    "        # to the old policy The old_policy can be run after the train_agent on\n",
    "        # the same workspace because it writes a logprob_predict and not an\n",
    "        # action. That is, it does not determine the action of the old_policy,\n",
    "        # it just determines the proba of the action of the current policy given\n",
    "        # its own probabilities\n",
    "\n",
    "        # Compute the critic value over the whole workspace\n",
    "        ppo.t_all_critics(train_workspace, t=0, n_steps=cfg.algorithm.n_steps)\n",
    "\n",
    "        ws_terminated, ws_reward, ws_v_value, ws_old_v_value = train_workspace[\n",
    "            \"env/terminated\",\n",
    "            \"env/reward\",\n",
    "            \"critic/v_values\",\n",
    "            \"old_critic/v_values\",\n",
    "        ]\n",
    "\n",
    "        # --- Critic optimization\n",
    "\n",
    "        # Avoids to extreme V-values (helps stability)\n",
    "        if cfg.algorithm.clip_range_vf > 0:\n",
    "            # Clip the difference between old and new values\n",
    "            # NOTE: this depends on the reward scaling\n",
    "            ws_v_value = ws_old_v_value + torch.clamp(\n",
    "                ws_v_value - ws_old_v_value,\n",
    "                -cfg.algorithm.clip_range_vf,\n",
    "                cfg.algorithm.clip_range_vf,\n",
    "            )\n",
    "\n",
    "        # Compute the advantage using the (clamped) critic values\n",
    "        with torch.no_grad():\n",
    "            advantage = gae(\n",
    "                ws_reward[1:],\n",
    "                ws_v_value[1:],\n",
    "                ~ws_terminated[1:],\n",
    "                ws_v_value[:-1],\n",
    "                cfg.algorithm.discount_factor,\n",
    "                cfg.algorithm.gae,\n",
    "            )\n",
    "        \n",
    "        # Compute the critic loss with TD(0)\n",
    "        target = ws_reward[1:] + cfg.algorithm.discount_factor * ws_old_v_value[1:].detach() * (1 - ws_terminated[1:].int())\n",
    "        critic_loss = torch.nn.functional.mse_loss(ws_v_value[:-1], target) * cfg.algorithm.critic_coef\n",
    "        ppo.critic_optimizer.zero_grad()\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            ppo.critic_agent.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        ppo.critic_optimizer.step()\n",
    "        \n",
    "        # --- Policy optimization\n",
    "\n",
    "        # We store the advantage into the train_workspace\n",
    "        if cfg.algorithm.normalize_advantage and advantage.shape[1] > 1:\n",
    "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "        train_workspace.set_full(\"advantage\", torch.cat(\n",
    "            (advantage, torch.zeros(1, advantage.shape[1]))\n",
    "        ))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Just computes the probability of the old policy's action\n",
    "            # to get the ratio of probabilities\n",
    "            t_old_policy(\n",
    "                train_workspace,\n",
    "                t=0,\n",
    "                n_steps=cfg.algorithm.n_steps,\n",
    "                predict_proba=True,\n",
    "                compute_entropy=False,\n",
    "            )\n",
    "\n",
    "        transition_workspace = train_workspace.get_transitions()\n",
    "        for opt_epoch in range(cfg.algorithm.opt_epochs):\n",
    "            if cfg.algorithm.batch_size > 0:\n",
    "                sample_workspace = transition_workspace.select_batch_n(\n",
    "                    cfg.algorithm.batch_size\n",
    "                )\n",
    "            else:\n",
    "                sample_workspace = transition_workspace\n",
    "\n",
    "            # Compute the policy loss\n",
    "\n",
    "            # Compute the KL divergence\n",
    "            kl = ...\n",
    "            # Compute the probability of the played actions according to the current policy\n",
    "            # We do not replay the action: we use the one stored into the dataset\n",
    "            # Note that the policy is not wrapped into a TemporalAgent, but we use a single step\n",
    "            #Compute the ratio of action probabilities\n",
    "            # Compute the policy loss\n",
    "            assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "            loss_policy = -cfg.algorithm.policy_coef * policy_loss\n",
    "\n",
    "            # Entropy loss favors exploration\n",
    "            # Note that the standard PPO algorithms do not have an entropy term, they don't need it\n",
    "            # because the KL term is supposed to deal with exploration\n",
    "            # So, to run the standard PPO algorithm, you should set cfg.algorithm.entropy_coef=0\n",
    "            assert len(entropy) == 1, f\"{entropy.shape}\"\n",
    "            entropy_loss = entropy[0].mean()\n",
    "            loss_entropy = -cfg.algorithm.entropy_coef * entropy_loss\n",
    "\n",
    "            # Store the losses for tensorboard display\n",
    "            ppo.logger.log_losses(critic_loss, entropy_loss, policy_loss, ppo.nb_steps)\n",
    "            ppo.logger.add_log(\"advantage\", policy_advantage.mean(), ppo.nb_steps)\n",
    "\n",
    "            ppo.policy_optimizer.zero_grad()\n",
    "            loss = loss_policy + loss_entropy\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                ppo.train_policy.parameters(), cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            ppo.policy_optimizer.step()\n",
    "\n",
    "        # Copy parameters\n",
    "        copy_parameters(ppo.train_policy, ppo.old_policy)\n",
    "        copy_parameters(ppo.critic_agent, ppo.old_critic_agent)\n",
    "        \n",
    "        # Evaluate\n",
    "        ppo.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f34be35",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Definition of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5546dd91",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"base_dir\": \"${gym_env.env_name}/ppo-kl-S${algorithm.seed}-b${algorithm.beta}_${current_time:}\",\n",
    "    \"save_best\": False,\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 12,\n",
    "        \"n_envs\": 8,\n",
    "        \"n_steps\": 32,\n",
    "        \"normalize_advantage\": False,\n",
    "        \"eval_interval\": 1000,\n",
    "        \"nb_evals\": 10,\n",
    "        \"gae\": 0.8,\n",
    "        \"discount_factor\": 0.98,\n",
    "        \"opt_epochs\": 10,\n",
    "        \"batch_size\": 256,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"policy_coef\": 1.,\n",
    "        \"beta\": 5.0,\n",
    "        \"clip_range_vf\": 0,\n",
    "        \"entropy_coef\": 2e-7,\n",
    "        \"critic_coef\": 1.0,\n",
    "        \"policy_type\": \"DiscretePolicy\",\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [64, 64],\n",
    "            \"critic_hidden_size\": [64, 64],\n",
    "        },\n",
    "        \"max_epochs\": 5000,\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPole-v1\",\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"eps\": 1e-5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecb846d",
   "metadata": {},
   "source": [
    "### Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7a4dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46150199",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.create(params)\n",
    "ppo_kl = PPOPenalty(cfg)\n",
    "run_ppo_penalty(ppo_kl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017233c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_kl.visualize_best()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
