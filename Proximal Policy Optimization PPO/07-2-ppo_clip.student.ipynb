{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "883d4e32",
   "metadata": {},
   "source": [
    " Copyright Â© Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13df05a",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook we code one version of the [Proximal Policy Optimization\n",
    "(PPO)](https://arxiv.org/pdf/1707.06347.pdf) algorithms using BBRL. More\n",
    "precisely, the version here is the one that clips the policy gradient.\n",
    "\n",
    "The PPO algorithm is superficially explained in [this\n",
    "video](https://www.youtube.com/watch?v=uRNL93jV2HE) and you can also read [the\n",
    "corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/ps/10_ppo.pdf).\n",
    "\n",
    "It is also a good idea to have a look at the [spinning up\n",
    "documentation](https://spinningup.openai.com/en/latest/algorithms/ppo.html).\n",
    "\n",
    "This version of PPO works, but it incorrectly samples minibatches randomly\n",
    "from the rollouts without making sure that each sample is used once and only\n",
    "once See:\n",
    "https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details/ for a\n",
    "full description of all the coding tricks that should be integrated\n",
    "\n",
    "# Setting up the environment\n",
    "We first need to setup the environment\n",
    "Installs the necessary Python and system libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f49f1c2",
   "metadata": {},
   "source": [
    "# Setting up the environment\n",
    "We first need to setup the environment\n",
    "Installs the necessary Python and system libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988beae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "    assert run([\"pip\", \"install\", \"easypip\"]).returncode == 0, \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup()\n",
    "\n",
    "# [[imports]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d7404c3",
   "metadata": {},
   "source": [
    "# Learning environment\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The learning environment is controlled by a configuration that define a few\n",
    "important things as described in the example below. This configuration can\n",
    "hold as many extra information as you need, the example below is the minimal\n",
    "one.\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    # This defines the a path for logs and saved models\n",
    "    \"base_dir\": \"${gym_env.env_name}/myalgo_${current_time:}\",\n",
    "\n",
    "    # The Gymnasium environment\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "\n",
    "    # Algorithm\n",
    "    \"algorithm\": {\n",
    "        # Seed used for the random number generator\n",
    "        \"seed\": 1023,\n",
    "\n",
    "        # Number of parallel training environments\n",
    "        \"n_envs\": 8,\n",
    "                \n",
    "        # Minimum number of steps between two evaluations\n",
    "        \"eval_interval\": 500,\n",
    "        \n",
    "        # Number of parallel evaluation environments\n",
    "        \"nb_evals\": 10,\n",
    "\n",
    "        # Number of epochs (loops)\n",
    "        \"max_epochs\": 40000,\n",
    "\n",
    "        # Number of steps (partial iteration)\n",
    "        \"n_steps\": 100,\n",
    "        \n",
    "    },\n",
    "}\n",
    "\n",
    "# Creates the configuration object, i.e. cfg.algorithm.nb_evals is 10\n",
    "cfg = OmegaConf.create(params)\n",
    "```\n",
    "\n",
    "## The RL algorithm\n",
    "\n",
    "In this notebook, the RL algorithm is based on `EpisodicAlgo`, that defines\n",
    "the algorithm environment when using episodes. To use such environment, we\n",
    "just need to subclass `EpisodicAlgo` and to define two things, namely the\n",
    "`train_policy` and the `eval_policy`. Both are BBRL agents that, given the\n",
    "environment state, select the action to perform.\n",
    "\n",
    "```py\n",
    "  class MyAlgo(EpisodicAlgo):\n",
    "      def __init__(self, cfg):\n",
    "          super().__init__(cfg)\n",
    "\n",
    "          # Define the train and evaluation policies\n",
    "          # (the agents compute the workspace `action` variable)\n",
    "          self.train_policy = MyPolicyAgent(...)\n",
    "          self.eval_policy = MyEvalAgent(...)\n",
    "\n",
    "algo = MyAlgo(cfg)\n",
    "```\n",
    "\n",
    "The `EpisodicAlgo` defines useful objects:\n",
    "\n",
    "- `algo.cfg` is the configuration\n",
    "- `algo.nb_steps` (integer) is the number of steps since the training began\n",
    "- `algo.logger` is a logger that can be used to collect statistics during training:\n",
    "    - `algo.logger.add_log(\"critic_loss\", critic_loss, algo.nb_steps)` registers the `critic_loss` value on tensorboard\n",
    "- `algo.evaluate()` evaluates the current `eval_policy` if needed, and keeps the\n",
    "agent if it was the best so far (average cumulated reward);\n",
    "- `algo.visualize_best()` runs the best agent on one episode, and displays the video\n",
    "\n",
    "\n",
    "\n",
    "Besides, it also defines an `iter_episodes` that allows to iterate over partial\n",
    "episodes (with `n_steps` from `n_envs` environments):\n",
    "\n",
    "```python3\n",
    "  # with partial episodes\n",
    "  for workspace in algo.iter_partial_episodes():\n",
    "      # workspace is a workspace containing 50 transitions\n",
    "      # (with autoreset)\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144a606b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Definition of PPO agents\n",
    "\n",
    "## Critic agent\n",
    "\n",
    "As A2C, PPO uses a value function $V(s)$. We thus call upon the `VAgent`\n",
    "class,  which takes an observation as input and whose output is the value of\n",
    "this observation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e02d8a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class VAgent(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, name=\"critic\"):\n",
    "        super().__init__(name)\n",
    "        self.is_q_function = False\n",
    "        self.model = build_ortho_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        observation = self.get((\"env/env_obs\", t))\n",
    "        critic = self.model(observation).squeeze(-1)\n",
    "        self.set((f\"{self.prefix}v_values\", t), critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b9ad67",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## The DiscretePolicy\n",
    "\n",
    "The DiscretePolicy was already used in A2C to deal with discrete actions, but\n",
    "we have added the possibility to only predict the probability of an action\n",
    "using the ```predict_proba``` variable in the ```forward()``` function. The\n",
    "code is as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8403f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiscretePolicy(Agent):\n",
    "    def __init__(self, state_dim, hidden_size, n_actions, name=\"policy\"):\n",
    "        super().__init__(name=name)\n",
    "        self.model = build_ortho_mlp(\n",
    "            [state_dim] + list(hidden_size) + [n_actions], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def dist(self, obs):\n",
    "        scores = self.model(obs)\n",
    "        probs = torch.softmax(scores, dim=-1)\n",
    "        return torch.distributions.Categorical(probs)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        t,\n",
    "        *,\n",
    "        stochastic=True,\n",
    "        predict_proba=False,\n",
    "        compute_entropy=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Compute the action given either a time step (looking into the workspace)\n",
    "        or an observation (in kwargs)\n",
    "        \"\"\"\n",
    "        observation = self.get((\"env/env_obs\", t))\n",
    "        scores = self.model(observation)\n",
    "        probs = torch.softmax(scores, dim=-1)\n",
    "\n",
    "        if predict_proba:\n",
    "            action = self.get((\"action\", t))\n",
    "            log_probs = probs[torch.arange(probs.size()[0]), action].log()\n",
    "            self.set((f\"{self.prefix}logprob_predict\", t), log_probs)\n",
    "        else:\n",
    "            if stochastic:\n",
    "                action = torch.distributions.Categorical(probs).sample()\n",
    "            else:\n",
    "                action = scores.argmax(1)\n",
    "            self.set((\"action\", t), action)\n",
    "\n",
    "        if compute_entropy:\n",
    "            entropy = torch.distributions.Categorical(probs).entropy()\n",
    "            self.set((f\"{self.prefix}entropy\", t), entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91e77f84",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Main PPO agent\n",
    "\n",
    "In the following, we create the PPO Agent, with one policy and one critic,\n",
    "and their \"delayed\" versions (target network for the critic, and previous \n",
    "policy in the inner loop of the optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3820f618",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "class PPOClip(EpisodicAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg, autoreset=True)\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "\n",
    "        self.train_policy = globals()[cfg.algorithm.policy_type](\n",
    "            obs_size,\n",
    "            cfg.algorithm.architecture.actor_hidden_size,\n",
    "            act_size,\n",
    "        ).with_prefix(\"current_policy/\")\n",
    "\n",
    "        self.eval_policy = KWAgentWrapper(\n",
    "            self.train_policy, \n",
    "            stochastic=False,\n",
    "            predict_proba=False,\n",
    "            compute_entropy=False,\n",
    "        )\n",
    "\n",
    "        self.critic_agent = VAgent(\n",
    "            obs_size, cfg.algorithm.architecture.critic_hidden_size\n",
    "        ).with_prefix(\"critic/\")\n",
    "        self.old_critic_agent = copy.deepcopy(self.critic_agent).with_prefix(\"old_critic/\")\n",
    "\n",
    "        self.old_policy = copy.deepcopy(self.train_policy)\n",
    "        self.old_policy.with_prefix(\"old_policy/\")\n",
    "\n",
    "        self.policy_optimizer = setup_optimizer(\n",
    "            cfg.optimizer, self.train_policy\n",
    "        )\n",
    "        self.critic_optimizer = setup_optimizer(\n",
    "            cfg.optimizer, self.critic_agent\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c60ac6",
   "metadata": {},
   "source": [
    "In the cell below, we optimize the policy loss for PPO-clip, i.e.\n",
    "\n",
    "$$\n",
    "L^{C L I P}(\\theta)=\\hat{\\mathbb{E}}_t\\left[\\min \\left(r_t(\\theta) \\hat{A}_t, \\operatorname{clip}\\left(r_t(\\theta), 1-\\epsilon, 1+\\epsilon\\right) \\hat{A}_t\\right)\\right]\n",
    "$$\n",
    "where $$r_t(\\theta) = \\frac{\\pi_\\theta\\left(a_t \\mid s_t\\right)}{\\pi_{\\theta_{\\text {old }}}\\left(a_t \\mid s_t\\right)}$$\n",
    "\n",
    "Useful torch functions:\n",
    "- [torch.clamp](https://pytorch.org/docs/stable/generated/torch.clamp.html) computes $\\min(\\max(x_i, m_i), M_i)$ where $m_i$ and $M_i$ are the lower and upper bounds respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ad1ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(ppo_clip: PPOClip):\n",
    "    cfg = ppo_clip.cfg\n",
    "\n",
    "    t_policy = TemporalAgent(ppo_clip.train_policy)\n",
    "    t_old_policy = TemporalAgent(ppo_clip.old_policy)\n",
    "    t_critic = TemporalAgent(ppo_clip.critic_agent)\n",
    "    t_old_critic = TemporalAgent(ppo_clip.old_critic_agent)\n",
    "\n",
    "    for train_workspace in iter_partial_episodes(\n",
    "        ppo_clip, cfg.algorithm.n_steps\n",
    "    ):\n",
    "        # Run the current policy and evaluate the proba of its action according\n",
    "        # to the old policy The old_policy can be run after the train_agent on\n",
    "        # the same workspace because it writes a logprob_predict and not an\n",
    "        # action. That is, it does not determine the action of the old_policy,\n",
    "        # it just determines the proba of the action of the current policy given\n",
    "        # its own probabilities\n",
    "\n",
    "        with torch.no_grad():\n",
    "            t_old_policy(\n",
    "                train_workspace,\n",
    "                t=0,\n",
    "                n_steps=cfg.algorithm.n_steps,\n",
    "                # Just computes the probability of the old policy's action\n",
    "                # to get the ratio of probabilities\n",
    "                predict_proba=True,\n",
    "                compute_entropy=False,\n",
    "            )\n",
    "\n",
    "        # Compute the critic value over the whole workspace\n",
    "        t_critic(train_workspace, t=0, n_steps=cfg.algorithm.n_steps)\n",
    "        with torch.no_grad():\n",
    "            t_old_critic(train_workspace, t=0, n_steps=cfg.algorithm.n_steps)\n",
    "\n",
    "        ws_terminated, ws_reward, ws_v_value, ws_old_v_value = train_workspace[\n",
    "            \"env/terminated\",\n",
    "            \"env/reward\",\n",
    "            \"critic/v_values\",\n",
    "            \"old_critic/v_values\",\n",
    "        ]\n",
    "\n",
    "        # the critic values are clamped to move not too far away from the values of the previous critic\n",
    "        if cfg.algorithm.clip_range_vf > 0:\n",
    "            # Clip the difference between old and new values\n",
    "            # NOTE: this depends on the reward scaling\n",
    "            ws_v_value = ws_old_v_value + torch.clamp(\n",
    "                ws_v_value - ws_old_v_value,\n",
    "                -cfg.algorithm.clip_range_vf,\n",
    "                cfg.algorithm.clip_range_vf,\n",
    "            )\n",
    "\n",
    "        # Compute the advantage using the (clamped) critic values\n",
    "        with torch.no_grad():\n",
    "            advantage = gae(\n",
    "                ws_reward[1:],\n",
    "                ws_v_value[1:],\n",
    "                ~ws_terminated[1:],\n",
    "                ws_v_value[:-1],\n",
    "                cfg.algorithm.discount_factor,\n",
    "                cfg.algorithm.gae,\n",
    "            )\n",
    "\n",
    "        ppo_clip.critic_optimizer.zero_grad()\n",
    "        target = ws_reward[1:] + cfg.algorithm.discount_factor * ws_old_v_value[1:].detach() * (1 - ws_terminated[1:].int())\n",
    "        critic_loss = torch.nn.functional.mse_loss(ws_v_value[:-1], target) * cfg.algorithm.critic_coef\n",
    "        critic_loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            ppo_clip.critic_agent.parameters(), cfg.algorithm.max_grad_norm\n",
    "        )\n",
    "        ppo_clip.critic_optimizer.step()\n",
    "\n",
    "        # We store the advantage into the transition_workspace\n",
    "        if cfg.algorithm.normalize_advantage and advantage.shape[1] > 1:\n",
    "            advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
    "        train_workspace.set_full(\"advantage\", torch.cat(\n",
    "            (advantage, torch.zeros(1, advantage.shape[1]))\n",
    "        ))\n",
    "        transition_workspace = train_workspace.get_transitions()\n",
    "\n",
    "        # Inner optimization loop: we sample transitions and use them to learn\n",
    "        # the policy\n",
    "        for opt_epoch in range(cfg.algorithm.opt_epochs):\n",
    "            if cfg.algorithm.batch_size > 0:\n",
    "                sample_workspace = transition_workspace.select_batch_n(\n",
    "                    cfg.algorithm.batch_size\n",
    "                )\n",
    "            else:\n",
    "                sample_workspace = transition_workspace\n",
    "\n",
    "            # Compute the policy loss\n",
    "\n",
    "            # Compute the probability of the played actions according to the current policy\n",
    "            # We do not replay the action: we use the one stored into the dataset\n",
    "            # Hence predict_proba=True\n",
    "            # Note that the policy is not wrapped into a TemporalAgent, but we use a single step\n",
    "            # Compute the ratio of action probabilities\n",
    "            # Compute the policy loss\n",
    "            # (using cfg.algorithm.clip_range and torch.clamp)\n",
    "            assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "            loss_policy = -cfg.algorithm.policy_coef * policy_loss\n",
    "\n",
    "            # Entropy loss favors exploration Note that the standard PPO\n",
    "            # algorithms do not have an entropy term, they don't need it because\n",
    "            # the KL term is supposed to deal with exploration So, to run the\n",
    "            # standard PPO algorithm, you should set\n",
    "            # cfg.algorithm.entropy_coef=0\n",
    "            assert len(entropy) == 1, f\"{entropy.shape}\"\n",
    "            entropy_loss = entropy[0].mean()\n",
    "            loss_entropy = -cfg.algorithm.entropy_coef * entropy_loss\n",
    "\n",
    "            # Store the losses for tensorboard display\n",
    "            ppo_clip.logger.log_losses(\n",
    "                critic_loss, entropy_loss, policy_loss, ppo_clip.nb_steps\n",
    "            )\n",
    "            ppo_clip.logger.add_log(\n",
    "                \"advantage\", policy_advantage[0].mean(), ppo_clip.nb_steps\n",
    "            )\n",
    "\n",
    "            loss = loss_policy + loss_entropy\n",
    "\n",
    "            ppo_clip.policy_optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(\n",
    "                ppo_clip.train_policy.parameters(), cfg.algorithm.max_grad_norm\n",
    "            )\n",
    "            ppo_clip.policy_optimizer.step()\n",
    "\n",
    "        # Copy parameters\n",
    "        copy_parameters(ppo_clip.train_policy, ppo_clip.old_policy)\n",
    "        copy_parameters(ppo_clip.critic_agent, ppo_clip.old_critic_agent)\n",
    "\n",
    "        # Evaluates our current algorithm if needed\n",
    "        ppo_clip.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b41d2de",
   "metadata": {},
   "source": [
    "# Definition of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215c7b59",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"base_dir\": \"${gym_env.env_name}/ppo-clip-S${algorithm.seed}_${current_time:}\",\n",
    "    \"save_best\": False,\n",
    "    \"logger\": {\n",
    "        \"classname\": \"bbrl.utils.logger.TFLogger\",\n",
    "        \"cache_size\": 10000,\n",
    "        \"every_n_seconds\": 10,\n",
    "        \"verbose\": False,\n",
    "    },\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 12,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"n_envs\": 8,\n",
    "        \"n_steps\": 32,\n",
    "        \"eval_interval\": 1000,\n",
    "        \"nb_evals\": 10,\n",
    "        \"gae\": 0.8,\n",
    "        \"discount_factor\": 0.98,\n",
    "        \"normalize_advantage\": False,\n",
    "        \"max_epochs\": 5_000,\n",
    "        \"opt_epochs\": 10,\n",
    "        \"batch_size\": 256,\n",
    "        \"clip_range\": 0.2,\n",
    "        \"clip_range_vf\": 0,\n",
    "        \"entropy_coef\": 2e-7,\n",
    "        \"policy_coef\": 1,\n",
    "        \"critic_coef\": 1.0,\n",
    "        \"policy_type\": \"DiscretePolicy\",\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [64, 64],\n",
    "            \"critic_hidden_size\": [64, 64],\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPole-v1\",\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"classname\": \"torch.optim.AdamW\",\n",
    "        \"lr\": 1e-3,\n",
    "        \"eps\": 1e-5,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5df820b",
   "metadata": {},
   "source": [
    "## Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58df3d8e",
   "metadata": {
    "title": "For Colab - otherwise, it is easier and better to launch tensorboard from"
   },
   "outputs": [],
   "source": [
    "# the terminal\n",
    "\n",
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e38b5fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_clip = PPOClip(OmegaConf.create(params))\n",
    "run(ppo_clip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60607c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppo_clip.visualize_best()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
