{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89718fa9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    " Copyright Â© Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63354a22",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook, we implement a simple version of the A2C algorithm using\n",
    "BBRL.\n",
    "\n",
    "To understand this code, you need to know more about [the BBRL interaction\n",
    "model](https://github.com/osigaud/bbrl/blob/master/docs/overview.md) Then you\n",
    "should run [a didactical\n",
    "example](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/02-multi_env_noautoreset.student.ipynb)\n",
    "to see how agents interact in BBRL when autoreset=False.\n",
    "\n",
    "The A2C algorithm is explained in [this\n",
    "video](https://www.youtube.com/watch?v=BUmsTlIgrBI) and you can also read [the\n",
    "corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/a2c.pdf). Here,\n",
    "we use it with autoreset=False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e36980",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils\").setup()\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent\n",
    "from bbrl_utils.algorithms import EpisodicAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from omegaconf import OmegaConf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c783e1",
   "metadata": {},
   "source": [
    "# Learning environment\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The learning environment is controlled by a configuration that define a few\n",
    "important things as described in the example below. This configuration can\n",
    "hold as many extra information as you need, the example below is the minimal\n",
    "one.\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    # This defines the a path for logs and saved models\n",
    "    \"base_dir\": \"${gym_env.env_name}/myalgo_${current_time:}\",\n",
    "\n",
    "    # The Gymnasium environment\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "\n",
    "    # Algorithm\n",
    "    \"algorithm\": {\n",
    "        # Seed used for the random number generator\n",
    "        \"seed\": 1023,\n",
    "\n",
    "        # Number of parallel training environments\n",
    "        \"n_envs\": 8,\n",
    "                \n",
    "        # Minimum number of steps between two evaluations\n",
    "        \"eval_interval\": 500,\n",
    "        \n",
    "        # Number of parallel evaluation environments\n",
    "        \"nb_evals\": 10,\n",
    "\n",
    "        # Number of epochs (loops)\n",
    "        \"max_epochs\": 40000,\n",
    "\n",
    "    },\n",
    "}\n",
    "\n",
    "# Creates the configuration object, i.e. cfg.algorithm.nb_evals is 10\n",
    "cfg = OmegaConf.create(params)\n",
    "```\n",
    "\n",
    "## The RL algorithm\n",
    "\n",
    "In this notebook, the RL algorithm is based on `EpisodicAlgo`, that defines\n",
    "the algorithm environment when using episodes. To use such environment, we\n",
    "just need to subclass `EpisodicAlgo` and to define two things, namely the\n",
    "`train_policy` and the `eval_policy`. Both are BBRL agents that, given the\n",
    "environment state, select the action to perform.\n",
    "\n",
    "```py\n",
    "  class MyAlgo(EpisodicAlgo):\n",
    "      def __init__(self, cfg):\n",
    "          super().__init__(cfg)\n",
    "\n",
    "          # Define the train and evaluation policies\n",
    "          # (the agents compute the workspace `action` variable)\n",
    "          self.train_policy = MyPolicyAgent(...)\n",
    "          self.eval_policy = MyEvalAgent(...)\n",
    "\n",
    "algo = MyAlgo(cfg)\n",
    "```\n",
    "\n",
    "The `EpisodicAlgo` defines useful objects:\n",
    "\n",
    "- `algo.cfg` is the configuration\n",
    "- `algo.nb_steps` (integer) is the number of steps since the training began\n",
    "- `algo.logger` is a logger that can be used to collect statistics during training:\n",
    "    - `algo.logger.add_log(\"critic_loss\", critic_loss, algo.nb_steps)` registers the `critic_loss` value on tensorboard\n",
    "- `algo.evaluate()` evaluates the current `eval_policy` if needed, and keeps the\n",
    "agent if it was the best so far (average cumulated reward);\n",
    "- `algo.visualize_best()` runs the best agent on one episode, and displays the video\n",
    "\n",
    "\n",
    "\n",
    "Besides, it also defines an `iter_episodes` is simple:\n",
    "\n",
    "```py\n",
    "  # With episodes\n",
    "  for workspace in rl_algo.iter_episodes():\n",
    "      # workspace is a workspace containing transitions\n",
    "      # Episodes shorter than the longer one contain duplicated\n",
    "      # transitions (with `env/done` set to true)\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127559ee",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## Definition of agents\n",
    "\n",
    "The [A2C](http://proceedings.mlr.press/v48/mniha16.pdf) algorithm is an\n",
    "actor-critic algorithm. Thus we need an Actor agent, a Critic agent and an\n",
    "Environment agent. Thus we need an Actor agent, a Critic agent and an\n",
    "Environment agent. The actor agents are built on an intermediate `ProbAgent`.\n",
    "Two agents that use the output of `ProbAgent` are defined below:\n",
    "- `ArgmaxActorAgent` that selects the action with the highest probability\n",
    "- `StochasticActorAgent` that selects the action using the probability\n",
    "  distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6e61e",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ProbAgent(Agent):\n",
    "    # Computes the distribution $p(a_t|s_t)$\n",
    "\n",
    "    def __init__(self, state_dim, hidden_layers, n_action, name=\"prob_agent\"):\n",
    "        super().__init__(name)\n",
    "        self.model = build_mlp(\n",
    "            [state_dim] + list(hidden_layers) + [n_action], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        # Get $s_t$\n",
    "        observation = self.get((\"env/env_obs\", t))\n",
    "        # Compute the distribution over actions\n",
    "        scores = self.model(observation)\n",
    "        action_probs = torch.softmax(scores, dim=-1)\n",
    "        assert not torch.any(torch.isnan(action_probs)), \"NaN Here\"\n",
    "\n",
    "        self.set((\"action_probs\", t), action_probs)\n",
    "        entropy = torch.distributions.Categorical(action_probs).entropy()\n",
    "        self.set((\"entropy\", t), entropy)\n",
    "\n",
    "\n",
    "class StochasticActorAgent(Agent):\n",
    "    \"\"\"Sample an action according to $p(a_t|s_t)$\"\"\"\n",
    "\n",
    "    def forward(self, t: int, **kwargs):\n",
    "        probs = self.get((\"action_probs\", t))\n",
    "        action = torch.distributions.Categorical(probs).sample()\n",
    "        self.set((\"action\", t), action)\n",
    "\n",
    "\n",
    "class ArgmaxActorAgent(Agent):\n",
    "    \"\"\"Choose an action $a$ that maximizes $p(a_t|s_t)\"\"\"\n",
    "\n",
    "    def forward(self, t: int, *, stochastic: bool = None, **kwargs):\n",
    "        probs = self.get((\"action_probs\", t))\n",
    "        action = probs.argmax(1)\n",
    "        self.set((\"action\", t), action)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68a1ebec",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### CriticAgent\n",
    "\n",
    "To implement the critic, A2C uses a value function $V(s)$. We thus call upon\n",
    "the `CriticAgent` class. The CriticAgent below is a one hidden layer neural\n",
    "network which takes an observation as input and whose output is the value of\n",
    "this observation. It thus implements a $V(s)$ function.\n",
    "\n",
    "It would be straightforward to define another CriticAgent (call it a\n",
    "CriticQAgent by contrast to a CriticAgent) that would take an observation and\n",
    "an action as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62228874",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CriticAgent(Agent):\n",
    "    def __init__(self, observation_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.critic_model = nn.Sequential(\n",
    "            nn.Linear(observation_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, t, **kwargs):\n",
    "        # Get the observation (shape B x O)\n",
    "        observation = self.get((\"env/env_obs\", t))\n",
    "        # The model outputs a matrix (shape B x 1) before squeeze transforms it\n",
    "        # to a vector (shape B)\n",
    "        critic = self.critic_model(observation).squeeze(-1)\n",
    "        self.set((\"critic\", t), critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd9b177",
   "metadata": {},
   "source": [
    "### Create the A2C environment\n",
    "\n",
    "In the piece of code below, we define the agents and other objects needed\n",
    "when training A2C (e.g. optimizers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6053512c",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create the A2C Agent\n",
    "class A2CAlgorithm(EpisodicAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "        observation_size, n_actions = self.train_env.get_obs_and_actions_sizes()\n",
    "\n",
    "        self.prob_agent = ProbAgent(\n",
    "            observation_size, cfg.algorithm.architecture.actor_hidden_size, n_actions\n",
    "        )\n",
    "        self.critic_agent = CriticAgent(\n",
    "            observation_size, cfg.algorithm.architecture.critic_hidden_size\n",
    "        )\n",
    "\n",
    "        # Define the train and evaluation agents\n",
    "        self.train_policy = Agents(self.prob_agent, StochasticActorAgent())\n",
    "        self.eval_policy = Agents(self.prob_agent, ArgmaxActorAgent())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10751ad9",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Compute critic loss\n",
    "\n",
    "In this basic version, the critic loss is computed by estimating the advantage\n",
    "as an expectation over the temporal difference error $\\delta$. This is not\n",
    "what the standard A2C algorithm does.\n",
    "\n",
    "You should use the `.detach()` in the computation of the temporal difference\n",
    "target. The idea is that we compute this target as a function of $V(s_{t+1})$,\n",
    "but we do not want to apply gradient descent on this $V(s_{t+1})$, we will\n",
    "only apply gradient descent to the $V(s_t)$ according to this target value.\n",
    "\n",
    "In practice, `x.detach()` detaches a computation graph from a tensor, so it\n",
    "avoids computing a gradient over this tensor.\n",
    "\n",
    "Note also the trick to deal with terminal states. If the state is terminal,\n",
    "$V(s_{t+1})$ does not make sense. Thus we need to ignore this term. So we\n",
    "multiply the term by `must_bootstrap`: if `must_bootstrap` is True (converted\n",
    "into an int, it becomes a 1), we get the term. If `must_bootstrap` is False\n",
    "(=0), we are at a terminal state, so we ignore the term. This trick is used in\n",
    "many RL libraries, e.g. SB3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02389c0b",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_critic_loss(cfg, reward, must_bootstrap, critic):\n",
    "    \"\"\"Returns a couple  TD(0) error ($\\delta_t$)\n",
    "\n",
    "    :param cfg: The configuration\n",
    "    :param reward: The reward (tensor 2xB)\n",
    "    :param must_bootstrap: The must bootstrap flag (tensor 2xB)\n",
    "    :param critic: The critic value (tensor 2xB)\n",
    "    :return: A couple (critic loss, $\\delta_t$)\n",
    "    \"\"\"\n",
    "    # Compute temporal difference\n",
    "\n",
    "    delta = ...\n",
    "    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "    # Compute critic loss\n",
    "    critic_loss = (delta**2).mean()\n",
    "    return critic_loss, delta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "820420fd",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "# Main training loop\n",
    "\n",
    "This version uses an AutoResetGymAgent. If you haven't done so yet, read\n",
    "[the BBRL documentation](https://github.com/osigaud/bbrl/blob/master/docs/overview.md)\n",
    "which explains a lot of details.\n",
    "\n",
    "Note that we `optimizer.zero_grad()`, `loss.backward()` and `optimizer.step()`\n",
    "lines. Several things need to be explained here.\n",
    "- `optimizer.zero_grad()` is necessary to cancel all the gradients computed at\n",
    "  the previous iterations\n",
    "- note that we sum all the losses, both for the critic and the actor, before\n",
    "applying back-propagation with `loss.backward()`. At first glance, summing\n",
    "these losses may look weird, as the actor and the critic receive different\n",
    "updates with different parts of the loss. This mechanism relies on the central\n",
    "property of tensor manipulation libraries like TensorFlow and pytorch. In\n",
    "pytorch, each loss tensor comes with its own graph of computation for\n",
    "back-propagating the gradient, in such a way that when you back-propagate the\n",
    "loss, the adequate part of the loss is applied to the adequate parameters.\n",
    "These mechanisms are partly explained\n",
    "[here](https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html).\n",
    "- since the optimizer has been set to work with both the actor and critic\n",
    "  parameters, `optimizer.step()` will optimize both agents and pytorch ensure\n",
    "  that each will receive its own part of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fedeb02",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def run_a2c(a2c: A2CAlgorithm):\n",
    "    cfg = a2c.cfg\n",
    "\n",
    "    # 4) Create the temporal critic agent to compute critic values over the workspace\n",
    "    t_critic_agent = TemporalAgent(a2c.critic_agent)\n",
    "\n",
    "    # 5) Configure the optimizer over the a2c agent\n",
    "    optimizer = setup_optimizer(cfg.optimizer, a2c.prob_agent, a2c.critic_agent)\n",
    "\n",
    "    for train_workspace in a2c.iter_episodes():\n",
    "        # Compute the critic value over the whole workspace\n",
    "        t_critic_agent(train_workspace, n_steps=train_workspace.time_size())\n",
    "\n",
    "        # Transform the episodes into transitions\n",
    "        transition_workspace = train_workspace.get_transitions()\n",
    "\n",
    "        # Get relevant tensors (size are T x B x ....)\n",
    "        critic, reward, action, action_probs, terminated = transition_workspace[\n",
    "            \"critic\",\n",
    "            \"env/reward\",\n",
    "            \"action\",\n",
    "            \"action_probs\",\n",
    "            \"env/terminated\",\n",
    "        ]\n",
    "\n",
    "        # Determines whether values of the critic should be propagated\n",
    "        # True if the episode reached a time limit or if the task was not done\n",
    "        # See https://github.com/osigaud/bbrl/blob/master/docs/time_limits.md\n",
    "        must_bootstrap = ~terminated\n",
    "\n",
    "        # To be completed...\n",
    "\n",
    "        # Compute the A2C loss\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "\n",
    "        # Evaluate if needed\n",
    "        a2c.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c368d7d5",
   "metadata": {},
   "source": [
    "## Definition of the parameters\n",
    "\n",
    "The logger is defined as `bbrl.utils.logger.TFLogger` so as to use a\n",
    "tensorboard visualisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7109e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"base_dir\": \"${gym_env.env_name}/a2c-S${algorithm.seed}_${current_time:}\",\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 432,\n",
    "        \"n_envs\": 10,\n",
    "        \"n_steps\": 16,\n",
    "        \"nb_measures\": 500,\n",
    "        \"nb_evals\": 20,\n",
    "        \"discount_factor\": 0.95,\n",
    "        # Number of transitions between two evaluations\n",
    "        \"eval_interval\": 1000,\n",
    "        \"max_epochs\": 800,\n",
    "        \"critic_coef\": 1.0,\n",
    "        \"actor_coef\": 0.1,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [32],\n",
    "            \"critic_hidden_size\": 32,\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPole-v1\",\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 0.01,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a2b68e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show tensorboard\n",
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a109fd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "a2c = A2CAlgorithm(OmegaConf.create(params))\n",
    "run_a2c(a2c)\n",
    "a2c.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf81e8a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "With the parameters provided in this notebook, you should observe that the reward\n",
    "is collapsing after 471 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fbc7b6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## And now... add some entropy\n",
    "\n",
    "To encourage the agent to explore more (or, said otherwise, to let the policy\n",
    "converge less quickly), you can add some entropy-based regularization.\n",
    "\n",
    "$$ \\mathcal{L}_{entropy} = \\mathbb{E}_{s \\sim \\pi_s}\\left( p(a | s) \\right) $$\n",
    "\n",
    "where $\\pi_s$ corresponds to the stationnary distribution according to the\n",
    "current policy $\\pi$ and the underlying MDP.\n",
    "\n",
    "You can use\n",
    "[`torch.distributions.Categorical`](https://pytorch.org/docs/stable/distributions.html#categorical)\n",
    "to quickly compute entropy for a categorical distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c6f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify A2C to add an entropic loss\n",
    "\n",
    "assert False, 'Not implemented yet'\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
