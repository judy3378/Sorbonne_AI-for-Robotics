{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a381e43",
   "metadata": {},
   "source": [
    " Copyright © Sorbonne University.\n",
    "\n",
    " This source code is licensed under the MIT license found in the LICENSE file\n",
    " in the root directory of this source tree."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d9e603",
   "metadata": {},
   "source": [
    "# Outlook\n",
    "\n",
    "In this notebook we code the Soft Actor-Critic (SAC) algorithm using BBRL.\n",
    "This algorithm is described in [this\n",
    "paper](http://proceedings.mlr.press/v80/haarnoja18b/haarnoja18b.pdf) and [this\n",
    "paper](https://arxiv.org/pdf/1812.05905.pdf).\n",
    "\n",
    "To understand this code, you need to know more about [the BBRL interaction\n",
    "model](https://github.com/osigaud/bbrl/blob/master/docs/overview.md) Then you\n",
    "should run [a didactical\n",
    "example](https://github.com/osigaud/bbrl/blob/master/docs/notebooks/03-multi_env_autoreset.student.ipynb)\n",
    "to see how agents interact in BBRL when autoreset=True.\n",
    "\n",
    "The algorithm is explained in [this\n",
    "video](https://www.youtube.com/watch?v=U20F-MvThjM) and you can also read [the\n",
    "corresponding slides](http://pages.isir.upmc.fr/~sigaud/teach/ps/12_sac.pdf).\n",
    "\n",
    "\n",
    "# Setting up the environment\n",
    "We first need to setup the environment\n",
    "Installs the necessary Python and system libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a474f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from easypip import easyimport\n",
    "except ModuleNotFoundError:\n",
    "    from subprocess import run\n",
    "\n",
    "    assert (\n",
    "        run([\"pip\", \"install\", \"easypip\"]).returncode == 0\n",
    "    ), \"Could not install easypip\"\n",
    "    from easypip import easyimport\n",
    "\n",
    "easyimport(\"swig\")\n",
    "easyimport(\"bbrl_utils>=0.5\").setup()\n",
    "\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from bbrl.workspace import Workspace\n",
    "from bbrl.agents import Agent, Agents, TemporalAgent, KWAgentWrapper\n",
    "from bbrl_utils.algorithms import EpochBasedAlgo\n",
    "from bbrl_utils.nn import build_mlp, setup_optimizer, soft_update_params\n",
    "from bbrl_utils.notebook import setup_tensorboard\n",
    "from omegaconf import OmegaConf\n",
    "from torch.distributions import (\n",
    "    Normal,\n",
    "    Independent,\n",
    "    TransformedDistribution,\n",
    "    TanhTransform,\n",
    ")\n",
    "import bbrl_gymnasium  # noqa: F401"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "459deec4",
   "metadata": {},
   "source": [
    "# Learning environment\n",
    "\n",
    "## Configuration\n",
    "\n",
    "The learning environment is controlled by a configuration that define a few\n",
    "important things as described in the example below. This configuration can\n",
    "hold as many extra information as you need, the example below is the minimal\n",
    "one.\n",
    "\n",
    "```python\n",
    "params = {\n",
    "    # This defines the a path for logs and saved models\n",
    "    \"base_dir\": \"${gym_env.env_name}/myalgo_${current_time:}\",\n",
    "\n",
    "    # The Gymnasium environment\n",
    "    \"gym_env\": {\n",
    "        \"env_name\": \"CartPoleContinuous-v1\",\n",
    "    },\n",
    "\n",
    "    # Algorithm\n",
    "    \"algorithm\": {\n",
    "        # Seed used for the random number generator\n",
    "        \"seed\": 1023,\n",
    "\n",
    "        # Number of parallel training environments\n",
    "        \"n_envs\": 8,\n",
    "                \n",
    "        # Minimum number of steps between two evaluations\n",
    "        \"eval_interval\": 500,\n",
    "        \n",
    "        # Number of parallel evaluation environments\n",
    "        \"nb_evals\": 10,\n",
    "\n",
    "        # Number of epochs (loops)\n",
    "        \"max_epochs\": 40000,\n",
    "\n",
    "        # Number of steps (partial iteration)\n",
    "        \"n_steps\": 100,\n",
    "        \n",
    "    },\n",
    "}\n",
    "\n",
    "# Creates the configuration object, i.e. cfg.algorithm.nb_evals is 10\n",
    "cfg = OmegaConf.create(params)\n",
    "```\n",
    "\n",
    "## The RL algorithm\n",
    "\n",
    "In this notebook, the RL algorithm is based on `EpisodicAlgo`, that defines\n",
    "the algorithm environment when using episodes. To use such environment, we\n",
    "just need to subclass `EpisodicAlgo` and to define two things, namely the\n",
    "`train_policy` and the `eval_policy`. Both are BBRL agents that, given the\n",
    "environment state, select the action to perform.\n",
    "\n",
    "```py\n",
    "  class MyAlgo(EpisodicAlgo):\n",
    "      def __init__(self, cfg):\n",
    "          super().__init__(cfg)\n",
    "\n",
    "          # Define the train and evaluation policies\n",
    "          # (the agents compute the workspace `action` variable)\n",
    "          self.train_policy = MyPolicyAgent(...)\n",
    "          self.eval_policy = MyEvalAgent(...)\n",
    "\n",
    "algo = MyAlgo(cfg)\n",
    "```\n",
    "\n",
    "The `EpisodicAlgo` defines useful objects:\n",
    "\n",
    "- `algo.cfg` is the configuration\n",
    "- `algo.nb_steps` (integer) is the number of steps since the training began\n",
    "- `algo.logger` is a logger that can be used to collect statistics during training:\n",
    "    - `algo.logger.add_log(\"critic_loss\", critic_loss, algo.nb_steps)` registers the `critic_loss` value on tensorboard\n",
    "- `algo.evaluate()` evaluates the current `eval_policy` if needed, and keeps the\n",
    "agent if it was the best so far (average cumulated reward);\n",
    "- `algo.visualize_best()` runs the best agent on one episode, and displays the video\n",
    "\n",
    "\n",
    "\n",
    "Besides, it also defines an `iter_episodes` that allows to iterate over partial\n",
    "episodes (with `n_steps` from `n_envs` environments):\n",
    "\n",
    "```python3\n",
    "  # with partial episodes\n",
    "  for workspace in algo.iter_partial_episodes():\n",
    "      # workspace is a workspace containing 50 transitions\n",
    "      # (with autoreset)\n",
    "      ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ceeeb27",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "## The SquashedGaussianActor\n",
    "\n",
    "SAC works better with a Squashed Gaussian actor, which transforms a gaussian\n",
    "distribution with a $tanh$. The computation of the gradient  uses the\n",
    "reparametrization trick. Note that our attempts to use a\n",
    "`TunableVarianceContinuousActor` as we did for instance in the notebook about\n",
    "PPO completely failed. Such failure is also documented in the [OpenAI spinning\n",
    "up documentation page about\n",
    "SAC](https://spinningup.openai.com/en/latest/algorithms/sac.html).\n",
    "\n",
    "The code of the `SquashedGaussianActor` actor is below.\n",
    "\n",
    "The fact that we use the reparametrization trick is hidden inside the code of\n",
    "this distribution. You can read more about the reparametrization trick in at\n",
    "the following URLs:\n",
    "- [Goker Erdogan's\n",
    "  blog](http://gokererdogan.github.io/2016/07/01/reparameterization-trick/)\n",
    "  which shows the variance of different tricks to compute gradient of\n",
    "  expectations for $\\mathbb{E}(x^2)$ where $x \\sim \\mathcal{N}(\\theta, 1)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30d1873",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class SquashedGaussianActor(Agent):\n",
    "    def __init__(self, state_dim, hidden_layers, action_dim, min_std=1e-4):\n",
    "        \"\"\"Creates a new Squashed Gaussian actor\n",
    "\n",
    "        :param state_dim: The dimension of the state space\n",
    "        :param hidden_layers: Hidden layer sizes\n",
    "        :param action_dim: The dimension of the action space\n",
    "        :param min_std: The minimum standard deviation, defaults to 1e-4\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.min_std = min_std\n",
    "        backbone_dim = [state_dim] + list(hidden_layers)\n",
    "        self.layers = build_mlp(backbone_dim, activation=nn.ReLU())\n",
    "        self.backbone = nn.Sequential(*self.layers)\n",
    "        self.last_mean_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
    "        self.last_std_layer = nn.Linear(hidden_layers[-1], action_dim)\n",
    "        self.softplus = nn.Softplus()\n",
    "        \n",
    "        # cache_size avoids numerical infinites or NaNs when\n",
    "        # computing log probabilities\n",
    "        self.tanh_transform = TanhTransform(cache_size=1)\n",
    "\n",
    "    def normal_dist(self, obs: torch.Tensor):\n",
    "        \"\"\"Compute normal distribution given observation(s)\"\"\"\n",
    "        \n",
    "        backbone_output = self.backbone(obs)\n",
    "        mean = self.last_mean_layer(backbone_output)\n",
    "        std_out = self.last_std_layer(backbone_output)\n",
    "        std = self.softplus(std_out) + self.min_std\n",
    "        # Independent ensures that we have a multivariate\n",
    "        # Gaussian with a diagonal covariance matrix (given as\n",
    "        # a vector `std`)\n",
    "        return Independent(Normal(mean, std), 1)\n",
    "\n",
    "    def forward(self, t, stochastic=True):\n",
    "        \"\"\"Computes the action a_t and its log-probability p(a_t| s_t)\n",
    "\n",
    "        :param stochastic: True when sampling\n",
    "        \"\"\"\n",
    "        normal_dist = self.normal_dist(self.get((\"env/env_obs\", t)))\n",
    "        action_dist = TransformedDistribution(normal_dist, [self.tanh_transform])\n",
    "        if stochastic:\n",
    "            # Uses the re-parametrization trick\n",
    "            action = action_dist.rsample()\n",
    "        else:\n",
    "            # Directly uses the mode of the distribution\n",
    "            action = self.tanh_transform(normal_dist.mode)\n",
    "\n",
    "        log_prob = action_dist.log_prob(action)\n",
    "        # This line allows to deepcopy the actor...\n",
    "        self.tanh_transform._cached_x_y = [None, None]\n",
    "        self.set((\"action\", t), action)\n",
    "        self.set((\"action_logprobs\", t), log_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c22206d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Critic agent Q(s,a)\n",
    "\n",
    "As critics and target critics, SAC uses several instances of ContinuousQAgent\n",
    "class, as DDPG and TD3. See the [DDPG\n",
    "notebook](http://master-dac.isir.upmc.fr/rld/rl/04-ddpg-td3.student.ipynb) for\n",
    "details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc68c0d",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "class ContinuousQAgent(Agent):\n",
    "    def __init__(self, state_dim: int, hidden_layers: list[int], action_dim: int):\n",
    "        \"\"\"Creates a new critic agent $Q(s, a)$\n",
    "\n",
    "        :param state_dim: The number of dimensions for the observations\n",
    "        :param hidden_layers: The list of hidden layers for the NN\n",
    "        :param action_dim: The numer of dimensions for actions\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.is_q_function = True\n",
    "        self.model = build_mlp(\n",
    "            [state_dim + action_dim] + list(hidden_layers) + [1], activation=nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, t):\n",
    "        obs = self.get((\"env/env_obs\", t))\n",
    "        action = self.get((\"action\", t))\n",
    "        obs_act = torch.cat((obs, action), dim=1)\n",
    "        q_value = self.model(obs_act).squeeze(-1)\n",
    "        self.set((f\"{self.prefix}q_value\", t), q_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8000cf30",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Building the complete training and evaluation agents\n",
    "\n",
    "In the code below we create the Squashed Gaussian actor, two critics and the\n",
    "corresponding target critics. Beforehand, we checked that the environment\n",
    "takes continuous actions (otherwise we would need a different code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8878a0d8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "# Create the SAC algorithm environment\n",
    "class SACAlgo(EpochBasedAlgo):\n",
    "    def __init__(self, cfg):\n",
    "        super().__init__(cfg)\n",
    "\n",
    "        obs_size, act_size = self.train_env.get_obs_and_actions_sizes()\n",
    "        assert (\n",
    "            self.train_env.is_continuous_action()\n",
    "        ), \"SAC code dedicated to continuous actions\"\n",
    "\n",
    "        # We need an actor\n",
    "        self.actor = SquashedGaussianActor(\n",
    "            obs_size, cfg.algorithm.architecture.actor_hidden_size, act_size\n",
    "        )\n",
    "\n",
    "        # Builds the critics\n",
    "        self.critic_1 = ContinuousQAgent(\n",
    "            obs_size,\n",
    "            cfg.algorithm.architecture.critic_hidden_size,\n",
    "            act_size,\n",
    "        ).with_prefix(\"critic-1/\")\n",
    "        self.target_critic_1 = copy.deepcopy(self.critic_1).with_prefix(\n",
    "            \"target-critic-1/\"\n",
    "        )\n",
    "\n",
    "        self.critic_2 = ContinuousQAgent(\n",
    "            obs_size,\n",
    "            cfg.algorithm.architecture.critic_hidden_size,\n",
    "            act_size,\n",
    "        ).with_prefix(\"critic-2/\")\n",
    "        self.target_critic_2 = copy.deepcopy(self.critic_2).with_prefix(\n",
    "            \"target-critic-2/\"\n",
    "        )\n",
    "\n",
    "        # Train and evaluation policies\n",
    "        self.train_policy = self.actor\n",
    "        self.eval_policy = KWAgentWrapper(self.actor, stochastic=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54b84789",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "For the entropy coefficient optimizer, the code is as follows. Note the trick\n",
    "which consists in using the log of this entropy coefficient. This trick was\n",
    "taken from the Stable baselines3 implementation of SAC, which is explained in\n",
    "[this\n",
    "notebook](https://colab.research.google.com/drive/12LER1_ShWOa_UhOL1nlX-LX_t5KQK9LV?usp=sharing).\n",
    "\n",
    "Tuning $\\alpha$ in SAC is an option. To chose to tune it, the `target_entropy`\n",
    "argument in the parameters should be `auto`. The initial value is given\n",
    "through the `entropy_coef` parameter. For any other value than `auto`, the\n",
    "value of $\\alpha$ will stay constant and correspond to the `entropy_coef`\n",
    "parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788e5da8",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def setup_entropy_optimizers(cfg):\n",
    "    if cfg.algorithm.entropy_mode == \"auto\":\n",
    "        # Note: we optimize the log of the entropy coef which is slightly different from the paper\n",
    "        # as discussed in https://github.com/rail-berkeley/softlearning/issues/37\n",
    "        # Comment and code taken from the SB3 version of SAC\n",
    "        log_entropy_coef = nn.Parameter(\n",
    "            torch.log(torch.ones(1) * cfg.algorithm.init_entropy_coef)\n",
    "        )\n",
    "        entropy_coef_optimizer = setup_optimizer(\n",
    "            cfg.entropy_coef_optimizer, log_entropy_coef\n",
    "        )\n",
    "        return entropy_coef_optimizer, log_entropy_coef\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a7434f",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Compute the critic loss\n",
    "\n",
    "With the notations of my slides, the equation corresponding to Eq. (5) and (6)\n",
    "in [this paper](https://arxiv.org/pdf/1812.05905.pdf) becomes:\n",
    "\n",
    "$$ loss_{Q_{\\boldsymbol{\\phi}_i}}({\\boldsymbol{\\theta}}) = {\\mathbb{E}}_{(\\mathbf{s}_t, \\mathbf{a}_t, \\mathbf{s}_{t+1}) \\sim\n",
    "\\mathcal{D}}\\left[\\left( r(\\mathbf{s}_t, \\mathbf{a}_t) + \\gamma {\\mathbb{E}}_{\\mathbf{a} \\sim\n",
    "\\pi_{\\boldsymbol{\\theta}}(.|\\mathbf{s}_{t+1})} \\left[\n",
    "\\min_{j\\in 1,2} \\hat{Q}^{\\mathrm{target}}_{\\boldsymbol{\\phi}_j}(\\mathbf{s}_{t+1}, \\mathbf{a}) - \\alpha\n",
    "\\log{\\pi_{\\boldsymbol{\\theta}}(\\mathbf{a}|\\mathbf{s}_{t+1})} \\right] - \\hat{Q}_{\\boldsymbol{\\phi}_i}(\\mathbf{s}_t, \\mathbf{a}_t) \\right)^2\n",
    "\\right] $$\n",
    "\n",
    "An important information in the above equation and the one about the actor\n",
    "loss below is the index of the expectations. These indexes tell us where the\n",
    "data should be taken from. In the above equation, one can see that the index\n",
    "of the outer expectation is over samples taken from the replay buffer, whereas\n",
    "in the inner expectation we consider actions from the current actor at the\n",
    "next state $s_{t+1}$.\n",
    "\n",
    "Thus, to compute the inner expectation, one needs to determine what actions\n",
    "the current actor would take in the next state of each sample. This is what\n",
    "the line\n",
    "\n",
    "`t_actor(rb_workspace, t=1, n_steps=1, stochastic=True)`\n",
    "\n",
    "does. The parameter `t=1` (instead of 0) ensures that we consider the next\n",
    "state $s_{t+1}$.\n",
    "\n",
    "Once we have determined these actions, we can determine their Q-values and\n",
    "their log probabilities, to compute the inner expectation.\n",
    "\n",
    "Note that at this stage, we only determine the log probabilities corresponding\n",
    "to actions taken at the next time step, by contrast with what we do for the\n",
    "actor in the `compute_actor_loss(...)` function later on.\n",
    "\n",
    "Finally, once we have computed the $$\n",
    "\\hat{Q}_{\\boldsymbol{\\phi}}(\\mathbf{s}_{t+1},\n",
    "\\mathbf{a}) $$ for both critics, we take the min and store it into\n",
    "`post_q_values`. By contrast, the Q-values corresponding to the last term of\n",
    "the equation are taken from the replay buffer, they are computed in the\n",
    "beginning of the function by applying the Q agents to the replay buffer\n",
    "*before* changing the action to that of the current actor.\n",
    "\n",
    "An important remark is that, if the entropy coefficient $\\alpha$ corresponding\n",
    "to the `ent_coef` variable is set to 0, then we retrieve exactly the critic\n",
    "loss computation function of the TD3 algorithm. As we will see later, this is\n",
    "also true of the actor loss computation.\n",
    "\n",
    "This remark proved very useful in debugging the SAC code. We have set\n",
    "`ent_coef` to 0 and ensured the behavior was strictly the same as the behavior\n",
    "of TD3.\n",
    "\n",
    "Note also that we compute the loss for two critics (initialized\n",
    "independently), and use two target critics (using the minimum of their\n",
    "prediction as the basis of the target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92bc2e8a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "def compute_critic_loss(\n",
    "    cfg,\n",
    "    reward: torch.Tensor,\n",
    "    must_bootstrap: torch.Tensor,\n",
    "    t_actor: TemporalAgent,\n",
    "    t_q_agents: TemporalAgent,\n",
    "    t_target_q_agents: TemporalAgent,\n",
    "    rb_workspace: Workspace,\n",
    "    ent_coef: torch.Tensor,\n",
    "):\n",
    "    r\"\"\"Computes the critic loss for a set of $S$ transition samples\n",
    "\n",
    "    Args:\n",
    "        cfg: The experimental configuration\n",
    "        reward: Tensor (2xS) of rewards\n",
    "        must_bootstrap: Tensor (2xS) of indicators\n",
    "        t_actor: The actor agent\n",
    "        t_q_agents: The critics\n",
    "        t_target_q_agents: The target of the critics\n",
    "        rb_workspace: The transition workspace\n",
    "        ent_coef: The entropy coefficient $\\alpha$\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: The two critic losses (scalars)\n",
    "    \"\"\"\n",
    "\n",
    "    # Replay the actor so we get the necessary statistics\n",
    "\n",
    "    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "    # Compute temporal difference\n",
    "\n",
    "    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "    return critic_loss_1, critic_loss_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a3bc70a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "### Compute the actor Loss\n",
    "\n",
    "With the notations of my slides, the equation of the actor loss corresponding\n",
    "to Eq. (7) in [this paper](https://arxiv.org/pdf/1812.05905.pdf) becomes:\n",
    "\n",
    "$$ loss_\\pi({\\boldsymbol{\\theta}}) = {\\mathbb{E}}_{\\mathbf{s}_t \\sim\n",
    "\\mathcal{D}}\\left[ {\\mathbb{E}}_{\\mathbf{a}_t\\sim\n",
    "\\pi_{\\boldsymbol{\\theta}}(.|\\mathbf{s}_t)} \\left[ \\alpha\n",
    "\\log{\\pi_{\\boldsymbol{\\theta}}(\\mathbf{a}_t|\\mathbf{s}_t) -\n",
    "\\hat{Q}_{\\boldsymbol{\\phi}_{i}}(\\mathbf{s}_t,\n",
    "\\mathbf{a}_t)} \\right] \\right] $$\n",
    "\n",
    "Note that [the paper](https://arxiv.org/pdf/1812.05905.pdf) mistakenly writes\n",
    "$Q_\\theta(s_t,s_t)$\n",
    "\n",
    "As for the critic loss, we have two expectations, one over the states from the\n",
    "replay buffer, and one over the actions of the current actor. Thus we need to\n",
    "apply again the current actor to the content of the replay buffer.\n",
    "\n",
    "But this time, we consider the current state, thus we parametrize it with\n",
    "`t=0` and `n_steps=1`. This way, we get the log probabilities and Q-values at\n",
    "the current step.\n",
    "\n",
    "A nice thing is that this way, there is no overlap between the log probability\n",
    "data used to update the critic and the actor, which avoids having to 'retain'\n",
    "the computation graph so that it can be reused for the actor and the critic.\n",
    "\n",
    "This small trick is one of the features that makes coding SAC the most\n",
    "difficult.\n",
    "\n",
    "Again, once we have computed the Q values over both critics, we take the min\n",
    "and put it into `current_q_values`.\n",
    "\n",
    "As for the critic loss, if we set `ent_coef` to 0, we retrieve the actor loss\n",
    "function of DDPG and TD3, which simply tries to get actions that maximize the\n",
    "Q values (by minimizing -Q)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43e31ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_actor_loss(\n",
    "    ent_coef, t_actor: TemporalAgent, t_q_agents: TemporalAgent, rb_workspace: Workspace\n",
    "):\n",
    "    r\"\"\"\n",
    "    Actor loss computation\n",
    "    :param ent_coef: The entropy coefficient $\\alpha$\n",
    "    :param t_actor: The actor agent (temporal agent)\n",
    "    :param t_q_agents: The critics (as temporal agent)\n",
    "    :param rb_workspace: The replay buffer (2 time steps, $t$ and $t+1$)\n",
    "    \"\"\"\n",
    "\n",
    "    # Recompute the action with the current actor (at $a_t$)\n",
    "\n",
    "    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "    # Compute Q-values\n",
    "\n",
    "    assert False, 'Not implemented yet'\n",
    "\n",
    "    current_q_values = torch.min(q_values_1, q_values_2)\n",
    "\n",
    "    # Compute the actor loss\n",
    "\n",
    "    # actor_loss =\n",
    "    assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "    return actor_loss.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe0257e",
   "metadata": {},
   "source": [
    "## Main training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9134daf6",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def run_sac(sac: SACAlgo):\n",
    "    cfg = sac.cfg\n",
    "    logger = sac.logger\n",
    "\n",
    "    \n",
    "    # init_entropy_coef is the initial value of the entropy coef alpha.\n",
    "    ent_coef = cfg.algorithm.init_entropy_coef\n",
    "    tau = cfg.algorithm.tau_target\n",
    "\n",
    "    # Creates the temporal actors\n",
    "    t_actor = TemporalAgent(sac.train_policy)\n",
    "    t_q_agents = TemporalAgent(Agents(sac.critic_1, sac.critic_2))\n",
    "    t_target_q_agents = TemporalAgent(Agents(sac.target_critic_1, sac.target_critic_2))\n",
    "\n",
    "    # Configure the optimizer\n",
    "    actor_optimizer = setup_optimizer(cfg.actor_optimizer, sac.actor)\n",
    "    critic_optimizer = setup_optimizer(cfg.critic_optimizer, sac.critic_1, sac.critic_2)\n",
    "    entropy_coef_optimizer, log_entropy_coef = setup_entropy_optimizers(cfg)\n",
    "\n",
    "\n",
    "    # If entropy_mode is not auto, the entropy coefficient ent_coef remains\n",
    "    # fixed. Otherwise, computes the target entropy\n",
    "    if cfg.algorithm.entropy_mode == \"auto\":\n",
    "        # target_entropy is \\mathcal{H}_0 in the SAC and aplications paper.\n",
    "        target_entropy = -np.prod(sac.train_env.action_space.shape).astype(np.float32)\n",
    "\n",
    "    # Loops over successive replay buffers\n",
    "    for rb in sac.iter_replay_buffers():\n",
    "        # Implement the SAC algorithm\n",
    "\n",
    "        # Critic update part #############################\n",
    "        # Actor update part #############################\n",
    "        assert False, 'Not implemented yet'\n",
    "\n",
    "\n",
    "        # Entropy optimizer part\n",
    "        if entropy_coef_optimizer is not None:\n",
    "            # See Eq. (17) of the SAC and Applications paper. The log\n",
    "            # probabilities *must* have been computed when computing the actor\n",
    "            # loss.\n",
    "            action_logprobs_rb = rb_workspace[\"action_logprobs\"].detach()\n",
    "            entropy_coef_loss = -(\n",
    "                log_entropy_coef.exp() * (action_logprobs_rb + target_entropy)\n",
    "            ).mean()\n",
    "            entropy_coef_optimizer.zero_grad()\n",
    "            entropy_coef_loss.backward()\n",
    "            entropy_coef_optimizer.step()\n",
    "            logger.add_log(\"entropy_coef_loss\", entropy_coef_loss, sac.nb_steps)\n",
    "            logger.add_log(\"entropy_coef\", ent_coef, sac.nb_steps)\n",
    "\n",
    "        ####################################################\n",
    "\n",
    "        # Soft update of target q function\n",
    "        soft_update_params(sac.critic_1, sac.target_critic_1, tau)\n",
    "        soft_update_params(sac.critic_2, sac.target_critic_2, tau)\n",
    "\n",
    "        agents.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bbea0a2",
   "metadata": {},
   "source": [
    "## Definition of the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b31c2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    \"save_best\": True,\n",
    "    \"base_dir\": \"${gym_env.env_name}/sac-S${algorithm.seed}_${current_time:}\",\n",
    "    \"algorithm\": {\n",
    "        \"seed\": 1,\n",
    "        \"n_envs\": 8,\n",
    "        \"n_steps\": 32,\n",
    "        \"buffer_size\": 1e6,\n",
    "        \"batch_size\": 256,\n",
    "        \"max_grad_norm\": 0.5,\n",
    "        \"nb_evals\": 16,\n",
    "        \"eval_interval\": 2_000,\n",
    "        \"learning_starts\": 10_000,\n",
    "        \"max_epochs\": 2_000,\n",
    "        \"discount_factor\": 0.98,\n",
    "        \"entropy_mode\": \"auto\",  # \"auto\" or \"fixed\"\n",
    "        \"init_entropy_coef\": 2e-7,\n",
    "        \"tau_target\": 0.05,\n",
    "        \"architecture\": {\n",
    "            \"actor_hidden_size\": [64, 64],\n",
    "            \"critic_hidden_size\": [256, 256],\n",
    "        },\n",
    "    },\n",
    "    \"gym_env\": {\"env_name\": \"CartPoleContinuous-v1\"},\n",
    "    \"actor_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "    \"critic_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "    \"entropy_coef_optimizer\": {\n",
    "        \"classname\": \"torch.optim.Adam\",\n",
    "        \"lr\": 3e-4,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa392a40",
   "metadata": {},
   "source": [
    "## Launching tensorboard to visualize the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f45903",
   "metadata": {},
   "outputs": [],
   "source": [
    "setup_tensorboard(\"./outputs/tblogs\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93017d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "agents = SACAlgo(OmegaConf.create(params))\n",
    "run_sac(agents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575932a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the best policy\n",
    "agents.visualize_best()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc6bc4",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "- use the same code on the Pendulum-v1 environment. This one is harder to\n",
    "  tune. Get the parameters from the\n",
    "  [rl-baseline3-zoo](https://github.com/DLR-RM/rl-baselines3-zoo) and see if\n",
    "  you manage to get SAC working on Pendulum"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
